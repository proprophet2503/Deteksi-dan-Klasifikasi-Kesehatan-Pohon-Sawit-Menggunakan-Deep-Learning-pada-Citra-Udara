{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOv76QDBIgyAQXGJsHQXNK8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"AEIGr7ahpRzc","executionInfo":{"status":"ok","timestamp":1751758050488,"user_tz":-420,"elapsed":5597,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}}},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.datasets import CocoDetection\n","from torchvision.transforms import functional as F\n","import matplotlib.pyplot as plt\n","from PIL import Image"]},{"cell_type":"code","source":["# Define transformations\n","class CocoTransform:\n","    def __call__(self, image, target):\n","        image = F.to_tensor(image)  # Convert PIL image to tensor\n","        return image, target"],"metadata":{"id":"0kUhvgFop1Qn","executionInfo":{"status":"ok","timestamp":1751758053981,"user_tz":-420,"elapsed":50,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3K-AtsuqHAH","executionInfo":{"status":"ok","timestamp":1751758083111,"user_tz":-420,"elapsed":27431,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"e284421f-dfd7-4542-d164-a71c293e4374"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def get_coco_dataset(img_dir, ann_file):\n","    return CocoDetection(\n","        root=img_dir,\n","        annFile=ann_file,\n","        transforms=CocoTransform()\n","    )\n","\n","train_dataset = get_coco_dataset(\n","    img_dir=\"/content/drive/MyDrive/FASTER RCNN SAWIT/faster rcnn palm tree.v3-pureahh.coco/train\",\n","    ann_file=\"/content/drive/MyDrive/FASTER RCNN SAWIT/faster rcnn palm tree.v3-pureahh.coco/train/annotations/_annotations.coco.json\"\n",")\n","\n","val_dataset = get_coco_dataset(\n","    img_dir=\"/content/drive/MyDrive/FASTER RCNN SAWIT/faster rcnn palm tree.v3-pureahh.coco/valid\",\n","    ann_file=\"/content/drive/MyDrive/FASTER RCNN SAWIT/faster rcnn palm tree.v3-pureahh.coco/valid/annotations/_annotations.coco.json\"\n",")\n","\n","\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLRnNM3Mp32F","executionInfo":{"status":"ok","timestamp":1751758087928,"user_tz":-420,"elapsed":2552,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"0eccab09-7578-45fd-8331-a7f93cfb4ec5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=1.73s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.70s)\n","creating index...\n","index created!\n"]}]},{"cell_type":"code","source":["def get_model(num_classes):\n","    # Load pre-trained Faster R-CNN\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","    # Get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","    # Replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model"],"metadata":{"id":"TYlRvvOCr6-l","executionInfo":{"status":"ok","timestamp":1751758091230,"user_tz":-420,"elapsed":15,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Initialize the model\n","num_classes = 4 # Background + sehat, normal, sakit\n","model = get_model(num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4JM7TQT9r-61","executionInfo":{"status":"ok","timestamp":1751758095965,"user_tz":-420,"elapsed":2941,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"8d6efaaf-8153-4eab-9ceb-3d4922d510fa"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:01<00:00, 98.5MB/s]\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-XbRptnsxMU","executionInfo":{"status":"ok","timestamp":1751758104338,"user_tz":-420,"elapsed":194,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"53e3fe3c-8f6b-4369-a4e7-8cc2f364caa8"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import torch\n","print(\"CUDA Available:\", torch.cuda.is_available())\n","print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZJs0WgG63Zu","executionInfo":{"status":"ok","timestamp":1751758580977,"user_tz":-420,"elapsed":7,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"7d971f59-a3a5-429d-d548-1851f8d42f1a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","GPU Name: Tesla T4\n"]}]},{"cell_type":"code","source":["params = [p for p in model.parameters() if p.requires_grad]\n","num_epochs = 30\n","# optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.AdamW(params, lr=0.001, weight_decay=0.0001)\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)"],"metadata":{"id":"MbNk3wMjsMSe","executionInfo":{"status":"ok","timestamp":1751758582329,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def train_one_epoch(model, optimizer, data_loader, device, epoch):\n","    model.train()\n","    for images, targets in data_loader:\n","        # Move images to the device\n","        images = [img.to(device) for img in images]\n","\n","        # Validate and process targets\n","        processed_targets = []\n","        valid_images = []\n","        for i, target in enumerate(targets):\n","            boxes = []\n","            labels = []\n","            for obj in target:\n","                # Extract bbox\n","                bbox = obj[\"bbox\"]  # Format: [x, y, width, height]\n","                x, y, w, h = bbox\n","\n","                # Ensure the width and height are positive\n","                if w > 0 and h > 0:\n","                    boxes.append([x, y, x + w, y + h])  # Convert to [x_min, y_min, x_max, y_max]\n","                    labels.append(obj[\"category_id\"])\n","\n","            # Only process if there are valid boxes\n","            if boxes:\n","                processed_target = {\n","                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n","                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n","                }\n","                processed_targets.append(processed_target)\n","                valid_images.append(images[i])  # Add only valid images\n","\n","        # Skip iteration if no valid targets\n","        if not processed_targets:\n","            continue\n","\n","        # Ensure images and targets are aligned\n","        images = valid_images\n","\n","        # Forward pass\n","        loss_dict = model(images, processed_targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch [{epoch}] Loss: {losses.item():.4f}\")"],"metadata":{"id":"XXZ5HZTTsR7j","executionInfo":{"status":"ok","timestamp":1751758584925,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","# num_epochs = 30\n","for epoch in range(num_epochs):\n","    train_one_epoch(model, optimizer, train_loader, device, epoch)\n","    lr_scheduler.step()\n","\n","    # Save the model's state dictionary after every epoch\n","    model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Model saved: {model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAr5S_WbsT-H","executionInfo":{"status":"ok","timestamp":1751758757093,"user_tz":-420,"elapsed":169864,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"991892df-fa8f-4007-8203-ff4492186e6f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0] Loss: 5.7217\n","Model saved: fasterrcnn_resnet50_epoch_1.pth\n","Epoch [1] Loss: 2.1866\n","Model saved: fasterrcnn_resnet50_epoch_2.pth\n","Epoch [2] Loss: 2.4137\n","Model saved: fasterrcnn_resnet50_epoch_3.pth\n","Epoch [3] Loss: 1.7140\n","Model saved: fasterrcnn_resnet50_epoch_4.pth\n","Epoch [4] Loss: 1.5265\n","Model saved: fasterrcnn_resnet50_epoch_5.pth\n","Epoch [5] Loss: 1.4891\n","Model saved: fasterrcnn_resnet50_epoch_6.pth\n","Epoch [6] Loss: 1.3367\n","Model saved: fasterrcnn_resnet50_epoch_7.pth\n","Epoch [7] Loss: 1.7495\n","Model saved: fasterrcnn_resnet50_epoch_8.pth\n","Epoch [8] Loss: 1.7073\n","Model saved: fasterrcnn_resnet50_epoch_9.pth\n","Epoch [9] Loss: 1.6560\n","Model saved: fasterrcnn_resnet50_epoch_10.pth\n","Epoch [10] Loss: 1.5194\n","Model saved: fasterrcnn_resnet50_epoch_11.pth\n","Epoch [11] Loss: 1.6520\n","Model saved: fasterrcnn_resnet50_epoch_12.pth\n","Epoch [12] Loss: 1.6315\n","Model saved: fasterrcnn_resnet50_epoch_13.pth\n","Epoch [13] Loss: 1.5233\n","Model saved: fasterrcnn_resnet50_epoch_14.pth\n","Epoch [14] Loss: 1.5963\n","Model saved: fasterrcnn_resnet50_epoch_15.pth\n","Epoch [15] Loss: 1.6051\n","Model saved: fasterrcnn_resnet50_epoch_16.pth\n","Epoch [16] Loss: 1.5572\n","Model saved: fasterrcnn_resnet50_epoch_17.pth\n","Epoch [17] Loss: 1.5015\n","Model saved: fasterrcnn_resnet50_epoch_18.pth\n","Epoch [18] Loss: 1.5449\n","Model saved: fasterrcnn_resnet50_epoch_19.pth\n","Epoch [19] Loss: 1.5407\n","Model saved: fasterrcnn_resnet50_epoch_20.pth\n","Epoch [20] Loss: 1.4978\n","Model saved: fasterrcnn_resnet50_epoch_21.pth\n","Epoch [21] Loss: 1.4490\n","Model saved: fasterrcnn_resnet50_epoch_22.pth\n","Epoch [22] Loss: 1.4481\n","Model saved: fasterrcnn_resnet50_epoch_23.pth\n","Epoch [23] Loss: 1.4948\n","Model saved: fasterrcnn_resnet50_epoch_24.pth\n","Epoch [24] Loss: 1.4846\n","Model saved: fasterrcnn_resnet50_epoch_25.pth\n","Epoch [25] Loss: 1.3869\n","Model saved: fasterrcnn_resnet50_epoch_26.pth\n","Epoch [26] Loss: 1.4943\n","Model saved: fasterrcnn_resnet50_epoch_27.pth\n","Epoch [27] Loss: 1.4682\n","Model saved: fasterrcnn_resnet50_epoch_28.pth\n","Epoch [28] Loss: 1.4131\n","Model saved: fasterrcnn_resnet50_epoch_29.pth\n","Epoch [29] Loss: 1.4597\n","Model saved: fasterrcnn_resnet50_epoch_30.pth\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.datasets import CocoDetection\n","from torchvision.transforms import functional as F\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# Load Faster R-CNN with ResNet-50 backbone\n","def get_model(num_classes):\n","    # Load pre-trained Faster R-CNN\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    # Get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # Replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model\n","\n","\n","# Initialize the model\n","num_classes = 4  # Background + sakit + normal  + sehat\n","\n","# Move model to GPU if available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","\n","# Load the trained model\n","model = get_model(num_classes)\n","model.load_state_dict(torch.load(\"fasterrcnn_resnet50_epoch_25.pth\"))\n","model.to(device)\n","model.eval()  # Set the model to evaluation mode\n","\n","\n","def prepare_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")  # Open image\n","    image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n","    return image_tensor.to(device)\n","\n","\n","import numpy as np\n","# Load the unseen image\n","image_path = '/content/drive/MyDrive/FASTER RCNN SAWIT/faster rcnn palm tree.v3-pureahh.coco/test/Stani_08_png.rf.db0a4535a0ddcee9c430d07b7291a4c8.jpg'\n","image_tensor = prepare_image(image_path)\n","\n","with torch.no_grad():  # Disable gradient computation for inference\n","    prediction = model(image_tensor)\n","\n","# `prediction` contains:\n","# - boxes: predicted bounding boxes\n","# - labels: predicted class labels\n","# - scores: predicted scores for each box (confidence level)\n","COCO_CLASSES = {0: \"Background\", 1: \"Normal\", 2: \"Sakit\", 3: \"Sehat\"}\n","\n","def get_class_name(class_id):\n","    return COCO_CLASSES.get(class_id, \"Unknown\")\n","\n","# Draw bounding boxes with the correct class names and increase image size\n","def draw_boxes(image, prediction, fig_size=(12, 10)):\n","    boxes = prediction[0]['boxes'].cpu().numpy()\n","    labels = prediction[0]['labels'].cpu().numpy()\n","    scores = prediction[0]['scores'].cpu().numpy()\n","\n","    threshold = 0.2  # Confidence threshold\n","\n","    # Define color for each class (in RGB)\n","    COLORS = {\n","        1: 'orange',   # Sakit\n","        2: 'deepskyblue',  # Normal\n","        3: 'purple'    # Sehat\n","    }\n","\n","    plt.figure(figsize=fig_size)\n","    plt.imshow(image)\n","\n","    for box, label, score in zip(boxes, labels, scores):\n","        if score > threshold:\n","            x_min, y_min, x_max, y_max = box\n","            color = COLORS.get(label, 'red')\n","            class_name = get_class_name(label)\n","\n","            # Draw rectangle and label\n","            plt.gca().add_patch(plt.Rectangle(\n","                (x_min, y_min), x_max - x_min, y_max - y_min,\n","                linewidth=2, edgecolor=color, facecolor='none'))\n","\n","            plt.text(x_min, y_min - 5, f\"{class_name} ({score:.2f})\",\n","                     color='black', fontsize=9,\n","                     bbox=dict(facecolor='white', alpha=0.6, edgecolor=color))\n","\n","    plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))\n","\n","# Display the image with bounding boxes and correct labels\n","# draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))  # Example of increased size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1sqAKqtlzF8NF0bw7I3DoOEvtd5PxDOov"},"id":"dfMh_jNd2Ozt","executionInfo":{"status":"ok","timestamp":1751758903318,"user_tz":-420,"elapsed":11815,"user":{"displayName":"Ἰερεμίας Jeremias","userId":"16467930393434775067"}},"outputId":"35fe8fde-8556-4762-adfb-f4e4dac13089"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}